{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install pdfplumber sentence-transformers faiss-cpu transformers torch requests\n",
    "\n",
    "# Import required modules\n",
    "import os\n",
    "import requests\n",
    "import faiss\n",
    "import pdfplumber\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Step 1: PDF Download\n",
    "def download_pdf(url, output_path):\n",
    "    \"\"\"\n",
    "    Downloads a PDF file from the provided URL and saves it to the specified path.\n",
    "    \"\"\"\n",
    "    print(f\"\\nDownloading PDF from: {url}\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(output_path, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                file.write(chunk)\n",
    "        print(f\"PDF downloaded successfully to: {output_path}\")\n",
    "    else:\n",
    "        print(\"Failed to download PDF. Please check the URL.\")\n",
    "        exit()\n",
    "\n",
    "# Step 2: PDF Text Extraction\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    \"\"\"\n",
    "    Extracts text from a given PDF file using pdfplumber.\n",
    "    \"\"\"\n",
    "    extracted_text = \"\"\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                extracted_text += page_text + \"\\n\"\n",
    "    return extracted_text.strip()\n",
    "\n",
    "# Step 3: Text Chunking\n",
    "def chunk_text(text, chunk_size=300, overlap=50):\n",
    "    \"\"\"\n",
    "    Splits the text into overlapping chunks for better granularity.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), chunk_size - overlap):\n",
    "        chunks.append(text[i:i + chunk_size])\n",
    "    return chunks\n",
    "\n",
    "# Step 4: Embedding and Vector Store Creation\n",
    "def create_vector_store(chunks, embedding_model_name=\"all-MiniLM-L6-v2\"):\n",
    "    \"\"\"\n",
    "    Converts text chunks into embeddings and stores them in a FAISS vector database.\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(embedding_model_name)\n",
    "    embeddings = model.encode(chunks, convert_to_tensor=True)\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings.cpu().numpy())  # FAISS requires numpy arrays\n",
    "    return index, model, chunks\n",
    "\n",
    "# Step 5: Query Retrieval and Response Generation\n",
    "def retrieve_relevant_chunks(query, index, chunks, model, top_k=5):\n",
    "    \"\"\"\n",
    "    Retrieves the most relevant chunks for a given query using FAISS similarity search.\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode([query], convert_to_tensor=True).cpu().numpy()\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    return [chunks[i] for i in indices[0] if i < len(chunks)]\n",
    "\n",
    "def generate_response(context, question, model_name=\"EleutherAI/gpt-neo-1.3B\"):\n",
    "    \"\"\"\n",
    "    Generates a direct response to the query using a pre-trained Hugging Face model.\n",
    "    \"\"\"\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "    # Set a pad token to avoid confusion (use eos_token as pad_token if necessary)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Format the input as a prompt\n",
    "    prompt = f\"Context:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True, padding=True)\n",
    "    \n",
    "    # Explicitly set the attention mask\n",
    "    attention_mask = inputs['attention_mask'].to(model.device)\n",
    "    input_ids = inputs['input_ids'].to(model.device)\n",
    "\n",
    "    # Generate response\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,  # Explicitly pass attention mask\n",
    "        max_new_tokens=150,\n",
    "        pad_token_id=tokenizer.eos_token_id  # Ensure padding doesn't interfere\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract the answer from the generated response\n",
    "    answer = response.split(\"Answer:\")[-1].strip()\n",
    "    return answer if answer else \"I couldn't find the exact answer. Please try rephrasing the question.\"\n",
    "\n",
    "\n",
    "# Step 6: PDF Processing and Query Handling\n",
    "def process_pdf_and_query(pdf_url, embedding_model_name=\"all-MiniLM-L6-v2\", language_model_name=\"EleutherAI/gpt-neo-1.3B\"):\n",
    "    \"\"\"\n",
    "    Downloads the PDF, processes it, creates a vector store, and answers user questions interactively.\n",
    "    \"\"\"\n",
    "    # Download the PDF\n",
    "    pdf_file = \"downloaded_pdf.pdf\"\n",
    "    download_pdf(pdf_url, pdf_file)\n",
    "\n",
    "    # Extract and process text\n",
    "    print(\"\\nExtracting text from the PDF...\")\n",
    "    text = extract_text_from_pdf(pdf_file)\n",
    "    print(\"Text extraction complete.\")\n",
    "\n",
    "    # Chunk text and create vector store\n",
    "    print(\"\\nChunking text and creating vector database...\")\n",
    "    chunks = chunk_text(text)\n",
    "    index, embed_model, all_chunks = create_vector_store(chunks, embedding_model_name)\n",
    "    print(\"Vector database created. Ready to answer questions.\")\n",
    "\n",
    "    # Interactive Q&A Loop\n",
    "    while True:\n",
    "        question = input(\"\\nEnter your question (or type 'exit' to quit): \").strip()\n",
    "        if question.lower() == \"exit\":\n",
    "            print(\"Exiting. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        relevant_chunks = retrieve_relevant_chunks(question, index, all_chunks, embed_model)\n",
    "        combined_context = \" \".join(relevant_chunks[:3])  # Combine top-3 chunks\n",
    "        print(\"\\nSearching for relevant information...\")\n",
    "\n",
    "        answer = generate_response(combined_context, question, language_model_name)\n",
    "        print(f\"\\nAnswer: {answer}\")\n",
    "\n",
    "# Step 7: Run the Pipeline\n",
    "def main():\n",
    "    pdf_url = \"https://www.hunter.cuny.edu/dolciani/pdf_files/workshop-materials/mmc-presentations/tables-charts-and-graphs-with-examples-from.pdf\"\n",
    "    process_pdf_and_query(pdf_url)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
